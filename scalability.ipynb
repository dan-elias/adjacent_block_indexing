{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index method scalability comparison\n",
    "\n",
    "This notebook compares the scalability properties of AdjacentBlockIndex with the BlockIndex and SortedNeighbourhoodIndex (both in the recordlinkage package).  \n",
    "\n",
    "\n",
    "\n",
    "Note: it should be run with memory paging disabled.  To do this on Linux, use:\n",
    "\n",
    "```\n",
    "sudo watch --interval 500 swapoff -a\n",
    "```\n",
    "\n",
    "Also, the kernel frequently gets killed, but the test function can continue from save points it makes as it goes.  A good way of restarting automatically (on Linux) is to export the notebook to a .py file\n",
    "(say, scalability.py) and then run:\n",
    "\n",
    "```\n",
    "while [ ! -f timings.pickle ]; do python scalability.py ; done\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from collections import ChainMap\n",
    "from functools import lru_cache, reduce\n",
    "from itertools import product\n",
    "from operator import mul\n",
    "import datetime, inspect, pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from recordlinkage import FullIndex, BlockIndex, SortedNeighbourhoodIndex\n",
    "from adjacent_block_index import AdjacentBlockIndex\n",
    "\n",
    "from experiment_helpers import sample_dedup_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_index_timings(index_details, row_counts, column_counts, distinct_entity_instance_counts, granularities, index_length_limit=5e6, index_time_limit=1, result_file=None, save_interval_seconds=60, verbose=False, debug=False):\n",
    "    '''\n",
    "    Negative granularities are neighbourhood radii\n",
    "    '''\n",
    "    if result_file is not None:\n",
    "        result_file = pathlib.Path(result_file).resolve()\n",
    "        partial_result_file = result_file.parent.joinpath('_{}'.format(result_file.name))\n",
    "        latest_keys_file = partial_result_file.parent.joinpath('_{}'.format(partial_result_file.name))\n",
    "    timings = pd.read_pickle(str(partial_result_file)) if (result_file is not None) and partial_result_file.exists() else pd.DataFrame()\n",
    "    failed_keys = pd.read_pickle(str(latest_keys_file)) if (result_file is not None) and latest_keys_file.exists() else {}\n",
    "    last_save_time = datetime.datetime.now()\n",
    "    def msg(*args, **kwargs):\n",
    "        if verbose:\n",
    "            print(*args, **kwargs)\n",
    "    def debug_msg(*args, **kwargs):\n",
    "        if debug:\n",
    "            print(*args, **kwargs)\n",
    "    total_combinations = reduce(mul, map(len, [index_details, row_counts, column_counts, distinct_entity_instance_counts, granularities]), 1)\n",
    "    combinations_processed = progress = 0\n",
    "    id_cols = ['instances_per_entity', 'granularity', 'rows', 'columns', 'label']\n",
    "    for instances_per_entity in sorted(distinct_entity_instance_counts):\n",
    "        msg('Instances per entity: {instances_per_entity}'.format(**locals()))\n",
    "        @lru_cache(maxsize=1)\n",
    "        def largest_continuous_dataset():\n",
    "            msg('Computing continuous dataset')\n",
    "            return sample_dedup_dataset(rows=max(row_counts), columns=max(column_counts), instances_per_entity=instances_per_entity)\n",
    "        for granularity in sorted(granularities, reverse=True):\n",
    "            msg('{progress:.0%}\\tGranularity: {granularity}'.format(**locals()))\n",
    "            @lru_cache(maxsize=1)\n",
    "            def largest_discrete_dataset():\n",
    "                msg('\\tDiscretizing dataset')\n",
    "                return np.floor(largest_continuous_dataset() * granularity)\n",
    "            for rows, columns in product(sorted(row_counts), sorted(column_counts)):\n",
    "                msg('{progress:.0%}\\t\\tTable: {rows} x {columns}'.format(**locals()))\n",
    "                @lru_cache(maxsize=2)\n",
    "                def dataset(dataset_type):\n",
    "                    largest_dataset = {'continuous': largest_continuous_dataset, 'discrete':largest_discrete_dataset}[dataset_type]\n",
    "                    return largest_dataset().iloc[:rows, :columns]\n",
    "                length_full = rows * (rows-1) // 2\n",
    "                for label, index_type, index_kwargs in index_details:\n",
    "                    index_kwargs = dict(index_kwargs)\n",
    "                    combinations_processed += 1\n",
    "                    progress = combinations_processed / total_combinations\n",
    "                    _locals = locals()\n",
    "                    current_keys = {k:_locals[k] for k in id_cols}\n",
    "                    def omission_contitions():\n",
    "                        debug_msg('\\t\\t\\t\\t{granularity} inapplicable to {label}?'.format(**locals()))\n",
    "                        yield ((index_type is not SortedNeighbourhoodIndex) and (granularity < 0))\n",
    "                        debug_msg('\\t\\t\\t\\t{granularity} granularity exceeds {rows} rows?'.format(**locals()))\n",
    "                        yield granularity > rows\n",
    "                        debug_msg('\\t\\t\\t\\t{columns} Too many columns?'.format(**locals()))\n",
    "                        yield ((index_type is SortedNeighbourhoodIndex) and (columns > 1))\n",
    "                        debug_msg('\\t\\t\\t\\tmax nulls too high?'.format(**locals()))\n",
    "                        yield index_kwargs.get('max_nulls', columns) > columns\n",
    "                        if len(timings) > 0:\n",
    "                            debug_msg('\\t\\t\\t\\tthis config already done?'.format(**locals()))\n",
    "                            yield reduce((lambda a,b: a&b), (timings[c]==_locals[c] for c in id_cols)).any()\n",
    "                            if index_type is FullIndex:\n",
    "                                debug_msg('\\t\\t\\t\\t{rows} rows already done for {label}?'.format(**locals()))\n",
    "                                yield (timings[timings['label']==label]['rows'] == rows).any()\n",
    "                            if index_type is SortedNeighbourhoodIndex:\n",
    "                                debug_msg('\\t\\t\\t\\t{rows} rows {granularity} granularity already done for {label}?'.format(**locals()))\n",
    "                                yield ((timings[timings['label']==label]['rows'] == rows) & (timings[timings['label']==label]['granularity'] == granularity)).any()\n",
    "                            comparison_rows = reduce((lambda a,b: a&b), \n",
    "                                                     (timings[c]==_locals[c] for c in id_cols if c != 'rows'),\n",
    "                                                     timings['rows'] <= rows,\n",
    "                                                    )\n",
    "                            if comparison_rows.any():\n",
    "                                comparison_maxima = timings[comparison_rows][['elapsed_seconds', 'index_length', 'memory_overflow']].max()\n",
    "                                debug_msg('\\t\\t\\t\\tsmaller index produced memory overflow?')\n",
    "                                yield comparison_maxima['memory_overflow'] \n",
    "                                if index_time_limit is not None:\n",
    "                                    debug_msg('\\t\\t\\t\\tsmaller index already exceeds time limit?')\n",
    "                                    yield comparison_maxima['elapsed_seconds'] > index_time_limit\n",
    "                                if index_length_limit is not None:\n",
    "                                    debug_msg('\\t\\t\\t\\tsmaller index already exceeds length limit?')\n",
    "                                    yield comparison_maxima['index_length'] > index_length_limit\n",
    "                    if any(omission_contitions()):\n",
    "                        debug_msg ('\\t\\t\\t\\t\\tskipping {label}'.format(**locals()))\n",
    "                        continue\n",
    "                    msg('{progress:.0%}\\t\\t\\t{label}'.format(**locals()))\n",
    "                    stats_dict = {k: _locals[k] for k in (['length_full'] + id_cols)}\n",
    "                    stats_dict.update(index_kwargs)\n",
    "                    if index_type is SortedNeighbourhoodIndex:\n",
    "                        dataset_type = 'continuous'\n",
    "                        index_kwargs['window'] = 1 + 2 * (int(rows / granularity / 2) if granularity > 0 else -granularity)\n",
    "                    else:\n",
    "                        dataset_type = 'discrete'\n",
    "                        stats_dict.update({k: _locals[k] for k in ['granularity']})\n",
    "                    if 'on' in inspect.signature(index_type).parameters:\n",
    "                        index_kwargs['on'] = list(dataset(dataset_type).columns)\n",
    "                    indexer = index_type(**index_kwargs)\n",
    "                    memory_overflow = (current_keys == failed_keys)\n",
    "                    elapsed_seconds = index_length = np.nan\n",
    "                    if not memory_overflow:\n",
    "                        if result_file is not None:\n",
    "                            pd.to_pickle(current_keys, str(latest_keys_file))\n",
    "                        try:\n",
    "                            start_time = datetime.datetime.now()\n",
    "                            index_length = len(indexer.index(dataset(dataset_type)))\n",
    "                            elapsed_seconds = (datetime.datetime.now() - start_time).total_seconds()\n",
    "                        except MemoryError:\n",
    "                            memory_overflow = True\n",
    "                    stats_dict.update({'index_length': index_length,\n",
    "                                       'elapsed_seconds': elapsed_seconds,\n",
    "                                       'memory_overflow': memory_overflow,\n",
    "                                      })\n",
    "                    timings = timings.append(stats_dict, ignore_index=True)\n",
    "                    if (result_file is not None) and (datetime.datetime.now() - last_save_time).total_seconds() > save_interval_seconds:\n",
    "                        timings.to_pickle(str(partial_result_file))\n",
    "                        last_save_time = datetime.datetime.now()\n",
    "    timings['reduction_ratio'] = 1 - timings['index_length'] / timings['length_full']\n",
    "    if result_file is not None:\n",
    "        timings.to_pickle(str(partial_result_file))\n",
    "        partial_result_file.rename(result_file)\n",
    "    progress = 1\n",
    "    msg('{progress:.0%}\\t Done'.format(**locals()))\n",
    "    return timings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_details = [#(label, type, additional kwargs),\n",
    "                 ('Full', FullIndex, {}),\n",
    "                 ('Standard Blocking',BlockIndex , {}),\n",
    "                 ('Sorted Neighbourhood', SortedNeighbourhoodIndex, {}),\n",
    "                 ('Adjacent Blocking - Standard Blocking settings',AdjacentBlockIndex , {'max_nulls': 0, 'ndx_sorting_keys':None}),\n",
    "                 ('Adjacent Blocking - no wildcards',AdjacentBlockIndex , {'max_nulls': 0}),\n",
    "                 ('Adjacent Blocking - 1 wildcard',AdjacentBlockIndex , {'max_nulls': 1}),\n",
    "                 ('Adjacent Blocking - 2 wildcards',AdjacentBlockIndex , {'max_nulls': 2}),\n",
    "                ]\n",
    "\n",
    "def int_geomspace(start, stop, num=50, endpoint=True):\n",
    "    return np.unique(np.logspace(np.log10(start), np.log10(stop), num=num, endpoint=endpoint, dtype=int))\n",
    "\n",
    "timings = get_index_timings(index_details=index_details,\n",
    "                        row_counts = int_geomspace(start=10, stop=1000000, num=16),\n",
    "                        column_counts = 1 + np.arange(10),\n",
    "                        distinct_entity_instance_counts = int_geomspace(start=1, stop=100, num=5),\n",
    "                        granularities = (-int_geomspace(start=1, stop=100, num=10)).tolist() + int_geomspace(start=1, stop=1000, num=10).tolist(),\n",
    "                        index_length_limit = 10e6,\n",
    "                        index_time_limit = 30,\n",
    "                        result_file = pathlib.Path('timings.pickle'),\n",
    "                        verbose = True,\n",
    "                        debug = False,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
